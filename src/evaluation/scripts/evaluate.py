from pathlib import Path
from .inference import calculate_inference
from .epoch_curves import plot_epoch_curves
from .precision_recall_f1_per_class import plot_prec_recall_f1_p_class
from .confusion_matrix import plot_confusion_matrix


def run_evaluate(output_dir, model_path, config, device):
    """
    Orchestrates the full evaluation pipeline based on training configurations and results.
    Loads the predicition outputs generated by inference on the test set and plots:
    - training, validation and learning-rate epoch curves
    - precision, recall and F1-scores per class
    - confusion matrix (absolute or normalized)

    All generated figures are saved to the specified figure directory.
    """
    PROJECT_ROOT = Path(__file__).resolve().parents[3]
    FIGURE_DIR = PROJECT_ROOT / "figures"
    FIGURE_DIR.mkdir(parents=True, exist_ok=True)

    print(f"\n[INFO] Starting evaluation for model: {model_path}")
    model_path = Path(model_path)

    # Assure that the latest model exists
    assert model_path.exists() and model_path.is_file()

    calculate_inference(output_dir, model_path, config, device)
    print(f"[INFO] Inference calculated.")

    # Plot accuracy, loss and learning rate
    plot_epoch_curves(FIGURE_DIR, csv_path="src/evaluation/outputs/epoch_metrics.csv")
    print(f"[INFO] Epoch curves drawn.")

    plot_prec_recall_f1_p_class(output_dir, FIGURE_DIR)
    print(f"[INFO] Precision, Recall and F1 per class diagramm created.")

    # normalized=False: absolute values, normalized=True: percentages
    plot_confusion_matrix(output_dir, FIGURE_DIR, normalized=True)
    print(f"[INFO] Confusion Matrix drawn.")
